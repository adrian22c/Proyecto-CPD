{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1bWyhkY0j4Srnb8m_ykFve15_wz4xXmWV",
      "authorship_tag": "ABX9TyO+1poBIEDvsSuZ9OynvJ4B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adrian22c/Proyecto-CPD/blob/main/Pipline_Proyecto_CPD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-y90syhpu9cN",
        "outputId": "3df9c5dc-6999-46cb-846c-f1f8a9acff8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightgbm dask-ml xgboost seaborn -q"
      ],
      "metadata": {
        "id": "B5-eTn_63xUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, warnings, datetime, json, joblib, time, psutil, gc\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd, numpy as np\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "from matplotlib.patches import Patch\n",
        "import dask.dataframe as dd\n",
        "from dask.distributed import Client\n",
        "from dask.diagnostics import ResourceProfiler, Profiler, CacheProfiler, ProgressBar\n",
        "from dask_ml.preprocessing import StandardScaler as DaskStandardScaler\n",
        "from dask_ml.model_selection import GridSearchCV as DaskGridSearchCV\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler as SKScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "import xgboost as xgb\n",
        "\n",
        "# Configuración de visualización\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.dpi'] = 300\n",
        "plt.rcParams['savefig.dpi'] = 300\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 1. CONFIGURACIÓN Y LOGGING\n",
        "# -----------------------------------------------------------\n",
        "class ExperimentLogger:\n",
        "    def __init__(self, base_dir):\n",
        "        self.base_dir = base_dir\n",
        "        self.log_file = f\"{base_dir}/experiment_log.txt\"\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def log(self, message):\n",
        "        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        log_msg = f\"[{timestamp}] {message}\"\n",
        "        print(log_msg)\n",
        "        with open(self.log_file, 'a') as f:\n",
        "            f.write(log_msg + \"\\n\")\n",
        "\n",
        "    def log_metrics(self, title, metrics_dict):\n",
        "        self.log(f\"\\n=== {title} ===\")\n",
        "        for key, value in metrics_dict.items():\n",
        "            if isinstance(value, float):\n",
        "                self.log(f\"{key}: {value:.6f}\")\n",
        "            else:\n",
        "                self.log(f\"{key}: {value}\")\n",
        "\n",
        "BASE = '/content/drive/MyDrive/CMAPSSData'\n",
        "RESULTS_DIR = f'{BASE}/paper_results'\n",
        "CLEAN = f'{BASE}/clean'\n",
        "FEAT_DIR = f'{BASE}/features'\n",
        "MODEL_DIR = f'{BASE}/models'\n",
        "PLOTS_DIR = f'{RESULTS_DIR}/plots'\n",
        "\n",
        "for d in [RESULTS_DIR, CLEAN, FEAT_DIR, MODEL_DIR, PLOTS_DIR]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "logger = ExperimentLogger(RESULTS_DIR)\n",
        "logger.log(\"=== INICIANDO EXPERIMENTO CIENTÍFICO ===\")\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 2. LECTURA Y PREPROCESAMIENTO\n",
        "# -----------------------------------------------------------\n",
        "def load_and_preprocess_data():\n",
        "    logger.log(\"Iniciando carga y preprocesamiento de datos...\")\n",
        "\n",
        "    COLS = ['unit_number','time_in_cycles'] + \\\n",
        "           [f'operational_setting_{i}' for i in range(1,4)] + \\\n",
        "           [f'sensor_measurement_{i}' for i in range(1,22)]\n",
        "\n",
        "    # Cargar datos con manejo de errores\n",
        "    try:\n",
        "        train = dd.read_csv(f'{BASE}/train_FD001.txt', header=None,\n",
        "                           delim_whitespace=True, names=COLS)\n",
        "        test = dd.read_csv(f'{BASE}/test_FD001.txt', header=None,\n",
        "                          delim_whitespace=True, names=COLS)\n",
        "        rul = pd.read_csv(f'{BASE}/RUL_FD001.txt', header=None, names=['RUL'])\n",
        "\n",
        "        logger.log(f\"Datos cargados - Train: {len(train)} filas, Test: {len(test)} filas\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log(f\"Error cargando datos: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Calcular RUL\n",
        "    train = train.assign(RUL=train.groupby('unit_number')['time_in_cycles'].transform('max') - train['time_in_cycles'])\n",
        "\n",
        "    # Análisis de datos constantes\n",
        "    sample = train.sample(frac=0.1).compute()  # Muestra más grande\n",
        "    const_cols = []\n",
        "    near_const_cols = []\n",
        "\n",
        "    for col in sample.columns:\n",
        "        if col in ['unit_number', 'time_in_cycles', 'RUL']:\n",
        "            continue\n",
        "        unique_vals = sample[col].nunique()\n",
        "        unique_ratio = unique_vals / len(sample)\n",
        "\n",
        "        if unique_vals <= 1:\n",
        "            const_cols.append(col)\n",
        "        elif unique_ratio < 0.01:  # Menos del 1% de valores únicos\n",
        "            near_const_cols.append(col)\n",
        "\n",
        "    logger.log(f\"Columnas constantes encontradas: {len(const_cols)}\")\n",
        "    logger.log(f\"Columnas casi constantes encontradas: {len(near_const_cols)}\")\n",
        "\n",
        "    # Eliminar columnas problemáticas\n",
        "    cols_to_drop = const_cols + near_const_cols\n",
        "    if cols_to_drop:\n",
        "        train = train.drop(columns=cols_to_drop)\n",
        "        test = test.drop(columns=cols_to_drop)\n",
        "        logger.log(f\"Eliminadas {len(cols_to_drop)} columnas problemáticas\")\n",
        "\n",
        "    # Guardar datos limpios\n",
        "    train.to_parquet(f'{CLEAN}/train_FD001_clean.parquet')\n",
        "    test.to_parquet(f'{CLEAN}/test_FD001_clean.parquet')\n",
        "    rul.to_csv(f'{CLEAN}/rul_FD001.csv', index=False)\n",
        "\n",
        "    return train, test, rul\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 3. INGENIERÍA DE CARACTERÍSTICAS\n",
        "# -----------------------------------------------------------\n",
        "def advanced_feature_engineering(train, test, rul):\n",
        "    logger.log(\"Iniciando ingeniería de características avanzada...\")\n",
        "\n",
        "    # Asegurar que trabajamos con pandas DataFrames para simplicidad\n",
        "    if hasattr(train, 'compute'):\n",
        "        # Es Dask DataFrame, convertir a pandas\n",
        "        logger.log(\"Convirtiendo Dask DataFrames a pandas...\")\n",
        "        train_pd = train.compute()\n",
        "        test_pd = test.compute()\n",
        "    else:\n",
        "        # Ya es pandas DataFrame\n",
        "        logger.log(\"Trabajando con pandas DataFrames...\")\n",
        "        train_pd = train\n",
        "        test_pd = test\n",
        "\n",
        "    # Selección de sensores basada en información mutua\n",
        "    sensor_cols = [c for c in train_pd.columns if 'sensor_measurement' in c]\n",
        "    sample = train_pd.sample(frac=0.2)  # Muestra más representativa (sin .compute())\n",
        "\n",
        "    # Calcular información mutua\n",
        "    mi_scores = mutual_info_regression(sample[sensor_cols], sample['RUL'], random_state=42)\n",
        "    mi_df = pd.DataFrame({'sensor': sensor_cols, 'mi_score': mi_scores}).sort_values('mi_score', ascending=False)\n",
        "\n",
        "    # Seleccionar top sensores y guardar análisis\n",
        "    threshold = np.percentile(mi_scores, 70)  # Top 30% de sensores\n",
        "    selected_sensors = mi_df[mi_df['mi_score'] > threshold]['sensor'].tolist()\n",
        "\n",
        "    logger.log(f\"Sensores seleccionados: {len(selected_sensors)} de {len(sensor_cols)}\")\n",
        "    mi_df.to_csv(f'{FEAT_DIR}/mutual_information_analysis.csv', index=False)\n",
        "\n",
        "    # Crear visualización de MI\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.bar(range(len(mi_df)), mi_df['mi_score'])\n",
        "    plt.axhline(y=threshold, color='r', linestyle='--', label=f'Threshold ({threshold:.3f})')\n",
        "    plt.xlabel('Sensores (ordenados por MI)')\n",
        "    plt.ylabel('Información Mutua')\n",
        "    plt.title('Análisis de Información Mutua - Selección de Sensores')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{PLOTS_DIR}/mutual_information_analysis.png', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Ingeniería de características con múltiples ventanas\n",
        "    WINDOWS = [3, 5, 10, 15]\n",
        "\n",
        "    def create_advanced_features(df, sensors, with_rul=True):\n",
        "        df = df.sort_values(['unit_number', 'time_in_cycles'])\n",
        "        base_cols = ['unit_number', 'time_in_cycles'] + (['RUL'] if with_rul else [])\n",
        "        result = df[base_cols].copy()\n",
        "\n",
        "        for sensor in sensors:\n",
        "            if sensor not in df.columns:\n",
        "                continue\n",
        "\n",
        "            # Características básicas\n",
        "            result[f'{sensor}_raw'] = df[sensor]\n",
        "\n",
        "            # Características de ventana móvil\n",
        "            for window in WINDOWS:\n",
        "                grouped = df.groupby('unit_number')[sensor]\n",
        "\n",
        "                # Estadísticas de ventana\n",
        "                result[f'{sensor}_mean_w{window}'] = grouped.rolling(window, min_periods=1).mean().reset_index(level=0, drop=True)\n",
        "                result[f'{sensor}_std_w{window}'] = grouped.rolling(window, min_periods=1).std().reset_index(level=0, drop=True)\n",
        "                result[f'{sensor}_min_w{window}'] = grouped.rolling(window, min_periods=1).min().reset_index(level=0, drop=True)\n",
        "                result[f'{sensor}_max_w{window}'] = grouped.rolling(window, min_periods=1).max().reset_index(level=0, drop=True)\n",
        "\n",
        "                # Tendencias\n",
        "                result[f'{sensor}_trend_w{window}'] = grouped.diff(window).reset_index(level=0, drop=True)\n",
        "\n",
        "            # Características de degradación\n",
        "            result[f'{sensor}_cumsum'] = grouped.cumsum().reset_index(level=0, drop=True)\n",
        "            result[f'{sensor}_diff'] = grouped.diff().fillna(0).reset_index(level=0, drop=True)\n",
        "\n",
        "        return result.fillna(0)\n",
        "\n",
        "    # Aplicar ingeniería de características usando pandas directamente\n",
        "    logger.log(\"Aplicando transformaciones de características...\")\n",
        "\n",
        "    train_features = create_advanced_features(train_pd, selected_sensors, True)\n",
        "    test_features = create_advanced_features(test_pd, selected_sensors, False)\n",
        "\n",
        "    # Agregar RUL real al conjunto de prueba\n",
        "    rul.index += 1\n",
        "    rul_dict = rul['RUL'].to_dict()\n",
        "    max_cycles = test_features.groupby('unit_number')['time_in_cycles'].transform('max')\n",
        "    test_features['RUL'] = test_features['unit_number'].map(rul_dict) - (max_cycles - test_features['time_in_cycles'])\n",
        "\n",
        "    # Guardar características\n",
        "    train_features.to_parquet(f'{FEAT_DIR}/train_features_advanced.parquet')\n",
        "    test_features.to_parquet(f'{FEAT_DIR}/test_features_advanced.parquet')\n",
        "\n",
        "    logger.log(f\"Características creadas - Train: {train_features.shape}, Test: {test_features.shape}\")\n",
        "\n",
        "    return train_features, test_features, selected_sensors\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 4. ANÁLISIS EXPLORATORIO DE DATOS (EDA)\n",
        "# -----------------------------------------------------------\n",
        "def comprehensive_eda(train_features, selected_sensors):\n",
        "    logger.log(\"Realizando análisis exploratorio de datos...\")\n",
        "\n",
        "    # 1. Distribución de RUL\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.hist(train_features['RUL'], bins=50, alpha=0.7, edgecolor='black')\n",
        "    plt.xlabel('RUL (ciclos)')\n",
        "    plt.ylabel('Frecuencia')\n",
        "    plt.title('Distribución de RUL')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    train_features.groupby('unit_number')['RUL'].max().hist(bins=30, alpha=0.7, edgecolor='black')\n",
        "    plt.xlabel('RUL Máximo por Unidad')\n",
        "    plt.ylabel('Frecuencia')\n",
        "    plt.title('RUL Máximo por Motor')\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    unit_cycles = train_features.groupby('unit_number')['time_in_cycles'].max()\n",
        "    plt.hist(unit_cycles, bins=30, alpha=0.7, edgecolor='black')\n",
        "    plt.xlabel('Ciclos Totales por Unidad')\n",
        "    plt.ylabel('Frecuencia')\n",
        "    plt.title('Duración de Vida por Motor')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{PLOTS_DIR}/rul_distribution_analysis.png', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # 2. Correlación entre sensores seleccionados\n",
        "    sensor_raw_cols = [f'{s}_raw' for s in selected_sensors if f'{s}_raw' in train_features.columns]\n",
        "    corr_matrix = train_features[sensor_raw_cols + ['RUL']].corr()\n",
        "\n",
        "    plt.figure(figsize=(14, 12))\n",
        "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "    sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
        "                square=True, fmt='.2f', cbar_kws={\"shrink\": .8})\n",
        "    plt.title('Matriz de Correlación - Sensores Seleccionados vs RUL')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{PLOTS_DIR}/correlation_matrix_selected_sensors.png', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # 3. Evolución temporal de sensores clave\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    sample_units = train_features['unit_number'].unique()[:6]\n",
        "    colors = plt.cm.tab10(np.linspace(0, 1, len(sample_units)))\n",
        "\n",
        "    for i, sensor in enumerate(selected_sensors[:6]):\n",
        "        sensor_col = f'{sensor}_raw'\n",
        "        if sensor_col not in train_features.columns:\n",
        "            continue\n",
        "\n",
        "        for unit, color in zip(sample_units, colors):\n",
        "            unit_data = train_features[train_features['unit_number'] == unit]\n",
        "            axes[i].plot(unit_data['time_in_cycles'], unit_data[sensor_col],\n",
        "                        alpha=0.7, color=color, linewidth=1)\n",
        "\n",
        "        axes[i].set_xlabel('Ciclos')\n",
        "        axes[i].set_ylabel('Valor del Sensor')\n",
        "        axes[i].set_title(f'{sensor} - Evolución Temporal')\n",
        "        axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{PLOTS_DIR}/sensor_temporal_evolution.png', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # 4. Estadísticas descriptivas\n",
        "    stats_cols = ['RUL'] + sensor_raw_cols\n",
        "    desc_stats = train_features[stats_cols].describe()\n",
        "    desc_stats.to_csv(f'{FEAT_DIR}/descriptive_statistics.csv')\n",
        "\n",
        "    logger.log(\"Análisis exploratorio completado\")\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 5. MODELADO CON MÚLTIPLES ALGORITMOS\n",
        "# -----------------------------------------------------------\n",
        "def comprehensive_modeling(train_features, test_features):\n",
        "    logger.log(\"Iniciando modelado comprehensivo...\")\n",
        "\n",
        "    # Preparar datos\n",
        "    feature_cols = [c for c in train_features.columns\n",
        "                   if c not in ['unit_number', 'time_in_cycles', 'RUL']]\n",
        "\n",
        "    X_train = train_features[feature_cols].fillna(0)\n",
        "    y_train = train_features['RUL']\n",
        "    X_test = test_features[feature_cols].fillna(0)\n",
        "    y_test = test_features['RUL']\n",
        "\n",
        "    logger.log(f\"Características utilizadas: {len(feature_cols)}\")\n",
        "    logger.log(f\"Datos de entrenamiento: {X_train.shape}\")\n",
        "    logger.log(f\"Datos de prueba: {X_test.shape}\")\n",
        "\n",
        "    # Normalización\n",
        "    scaler = SKScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Definir modelos\n",
        "    models = {\n",
        "        'LightGBM': LGBMRegressor(objective='regression', random_state=42, n_jobs=-1, verbose=-1),\n",
        "        'XGBoost': xgb.XGBRegressor(random_state=42, n_jobs=-1, verbosity=0),\n",
        "        'RandomForest': RandomForestRegressor(random_state=42, n_jobs=-1),\n",
        "        'Ridge': Ridge(random_state=42)\n",
        "    }\n",
        "\n",
        "    # Parámetros para búsqueda\n",
        "    param_grids = {\n",
        "        'LightGBM': {\n",
        "            'learning_rate': [0.01, 0.1, 0.2],\n",
        "            'n_estimators': [100, 200, 500],\n",
        "            'max_depth': [3, 6, 9],\n",
        "            'num_leaves': [31, 50, 100]\n",
        "        },\n",
        "        'XGBoost': {\n",
        "            'learning_rate': [0.01, 0.1, 0.2],\n",
        "            'n_estimators': [100, 200, 500],\n",
        "            'max_depth': [3, 6, 9]\n",
        "        },\n",
        "        'RandomForest': {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'max_depth': [10, 20, None],\n",
        "            'min_samples_split': [2, 5, 10]\n",
        "        },\n",
        "        'Ridge': {\n",
        "            'alpha': [0.1, 1.0, 10.0, 100.0]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    trained_models = {}\n",
        "\n",
        "    # Validación cruzada temporal\n",
        "    tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        logger.log(f\"\\n--- Entrenando {model_name} ---\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Usar datos escalados para Ridge, datos originales para tree-based\n",
        "        X_train_model = X_train_scaled if model_name == 'Ridge' else X_train\n",
        "        X_test_model = X_test_scaled if model_name == 'Ridge' else X_test\n",
        "\n",
        "        try:\n",
        "            # Búsqueda de hiperparámetros\n",
        "            from sklearn.model_selection import GridSearchCV\n",
        "            grid_search = GridSearchCV(\n",
        "                model, param_grids[model_name],\n",
        "                cv=tscv, scoring='neg_root_mean_squared_error',\n",
        "                n_jobs=-1, verbose=0\n",
        "            )\n",
        "\n",
        "            grid_search.fit(X_train_model, y_train)\n",
        "            best_model = grid_search.best_estimator_\n",
        "\n",
        "            # Predicción\n",
        "            y_pred = best_model.predict(X_test_model)\n",
        "\n",
        "            # Métricas\n",
        "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "            mae = mean_absolute_error(y_test, y_pred)\n",
        "            r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "            # Validación cruzada\n",
        "            cv_scores = cross_val_score(best_model, X_train_model, y_train,\n",
        "                                      cv=tscv, scoring='neg_root_mean_squared_error')\n",
        "            cv_rmse_mean = -cv_scores.mean()\n",
        "            cv_rmse_std = cv_scores.std()\n",
        "\n",
        "            training_time = time.time() - start_time\n",
        "\n",
        "            results[model_name] = {\n",
        "                'rmse': rmse,\n",
        "                'mae': mae,\n",
        "                'r2': r2,\n",
        "                'cv_rmse_mean': cv_rmse_mean,\n",
        "                'cv_rmse_std': cv_rmse_std,\n",
        "                'training_time': training_time,\n",
        "                'best_params': grid_search.best_params_,\n",
        "                'predictions': y_pred\n",
        "            }\n",
        "\n",
        "            trained_models[model_name] = best_model\n",
        "\n",
        "            logger.log_metrics(f\"{model_name} Resultados\", {\n",
        "                'RMSE': rmse,\n",
        "                'MAE': mae,\n",
        "                'R²': r2,\n",
        "                'CV RMSE (mean±std)': f\"{cv_rmse_mean:.3f}±{cv_rmse_std:.3f}\",\n",
        "                'Tiempo entrenamiento': f\"{training_time:.2f}s\"\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.log(f\"Error entrenando {model_name}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return results, trained_models, y_test, scaler\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 6. ANÁLISIS DE RENDIMIENTO DASK vs PANDAS\n",
        "# -----------------------------------------------------------\n",
        "def dask_vs_pandas_comparison(train_features, test_features):\n",
        "    logger.log(\"Comparando rendimiento Dask vs Pandas...\")\n",
        "\n",
        "    feature_cols = [c for c in train_features.columns\n",
        "                   if c not in ['unit_number', 'time_in_cycles', 'RUL']]\n",
        "\n",
        "    comparison_results = {}\n",
        "\n",
        "    # Preparar datos para ambos enfoques\n",
        "    X_train_pd = train_features[feature_cols].fillna(0)\n",
        "    y_train_pd = train_features['RUL']\n",
        "    X_test_pd = test_features[feature_cols].fillna(0)\n",
        "    y_test_pd = test_features['RUL']\n",
        "\n",
        "    # Convertir a Dask\n",
        "    X_train_dd = dd.from_pandas(X_train_pd, npartitions=4)\n",
        "    y_train_dd = dd.from_pandas(y_train_pd, npartitions=4)\n",
        "    X_test_dd = dd.from_pandas(X_test_pd, npartitions=2)\n",
        "\n",
        "    # Modelo simple para comparación\n",
        "    model_params = {'n_estimators': 100, 'max_depth': 6, 'random_state': 42, 'n_jobs': -1}\n",
        "\n",
        "    # ENFOQUE PANDAS (Eager)\n",
        "    logger.log(\"Probando enfoque Pandas (eager)...\")\n",
        "    start_time = time.time()\n",
        "    memory_before = psutil.virtual_memory().used / 1e9\n",
        "\n",
        "    scaler_pd = SKScaler()\n",
        "    X_train_scaled_pd = scaler_pd.fit_transform(X_train_pd)\n",
        "    X_test_scaled_pd = scaler_pd.transform(X_test_pd)\n",
        "\n",
        "    model_pd = LGBMRegressor(**model_params, verbose=-1)\n",
        "    model_pd.fit(X_train_scaled_pd, y_train_pd)\n",
        "    y_pred_pd = model_pd.predict(X_test_scaled_pd)\n",
        "\n",
        "    pandas_time = time.time() - start_time\n",
        "    memory_after = psutil.virtual_memory().used / 1e9\n",
        "    pandas_memory = memory_after - memory_before\n",
        "\n",
        "    pandas_rmse = np.sqrt(mean_squared_error(y_test_pd, y_pred_pd))\n",
        "    pandas_r2 = r2_score(y_test_pd, y_pred_pd)\n",
        "\n",
        "    # ENFOQUE DASK (Lazy)\n",
        "    logger.log(\"Probando enfoque Dask (lazy)...\")\n",
        "    start_time = time.time()\n",
        "    memory_before = psutil.virtual_memory().used / 1e9\n",
        "\n",
        "    # Usar Dask para normalización\n",
        "    scaler_dask = DaskStandardScaler()\n",
        "    X_train_scaled_dd = scaler_dask.fit_transform(X_train_dd)\n",
        "    X_test_scaled_dd = scaler_dask.transform(X_test_dd)\n",
        "\n",
        "    # Usar búsqueda distribuida\n",
        "    tscv = TimeSeriesSplit(n_splits=3)  # Reducido para velocidad\n",
        "    param_grid = {'learning_rate': [0.1], 'n_estimators': [100]}\n",
        "\n",
        "    dask_model = DaskGridSearchCV(\n",
        "        LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1),\n",
        "        param_grid, cv=tscv, scoring='neg_root_mean_squared_error'\n",
        "    )\n",
        "\n",
        "    with ResourceProfiler(), Profiler():\n",
        "        dask_model.fit(X_train_scaled_dd, y_train_dd)\n",
        "        y_pred_dask = dask_model.predict(X_test_scaled_dd.compute())\n",
        "\n",
        "    dask_time = time.time() - start_time\n",
        "    memory_after = psutil.virtual_memory().used / 1e9\n",
        "    dask_memory = memory_after - memory_before\n",
        "\n",
        "    dask_rmse = np.sqrt(mean_squared_error(y_test_pd, y_pred_dask))\n",
        "    dask_r2 = r2_score(y_test_pd, y_pred_dask)\n",
        "\n",
        "    # Guardar resultados de comparación\n",
        "    comparison_results = {\n",
        "        'pandas': {\n",
        "            'time': pandas_time,\n",
        "            'memory_gb': pandas_memory,\n",
        "            'rmse': pandas_rmse,\n",
        "            'r2': pandas_r2,\n",
        "            'approach': 'eager'\n",
        "        },\n",
        "        'dask': {\n",
        "            'time': dask_time,\n",
        "            'memory_gb': dask_memory,\n",
        "            'rmse': dask_rmse,\n",
        "            'r2': dask_r2,\n",
        "            'approach': 'lazy'\n",
        "        }\n",
        "    }\n",
        "\n",
        "    logger.log_metrics(\"Comparación Pandas vs Dask\", {\n",
        "        'Pandas - Tiempo': f\"{pandas_time:.2f}s\",\n",
        "        'Pandas - Memoria': f\"{pandas_memory:.2f}GB\",\n",
        "        'Pandas - RMSE': f\"{pandas_rmse:.4f}\",\n",
        "        'Dask - Tiempo': f\"{dask_time:.2f}s\",\n",
        "        'Dask - Memoria': f\"{dask_memory:.2f}GB\",\n",
        "        'Dask - RMSE': f\"{dask_rmse:.4f}\",\n",
        "        'Speedup': f\"{pandas_time/dask_time:.2f}x\" if dask_time > 0 else \"N/A\"\n",
        "    })\n",
        "\n",
        "    return comparison_results\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 7. ANÁLISIS DE ESCALABILIDAD\n",
        "# -----------------------------------------------------------\n",
        "def scalability_analysis():\n",
        "    logger.log(\"Realizando análisis de escalabilidad...\")\n",
        "\n",
        "    # Configuraciones de workers a probar\n",
        "    worker_configs = [\n",
        "        (1, 1),   # 1 worker, 1 thread\n",
        "        (2, 1),   # 2 workers, 1 thread cada uno\n",
        "        (2, 2),   # 2 workers, 2 threads cada uno\n",
        "        (4, 1),   # 4 workers, 1 thread cada uno\n",
        "        (4, 2),   # 4 workers, 2 threads cada uno\n",
        "    ]\n",
        "\n",
        "    scalability_results = []\n",
        "\n",
        "    # Datos sintéticos para prueba controlada\n",
        "    np.random.seed(42)\n",
        "    n_samples = 50000\n",
        "    n_features = 50\n",
        "\n",
        "    X_synthetic = np.random.randn(n_samples, n_features)\n",
        "    y_synthetic = np.random.randn(n_samples)\n",
        "\n",
        "    for n_workers, threads_per_worker in worker_configs:\n",
        "        logger.log(f\"Probando configuración: {n_workers} workers, {threads_per_worker} threads c/u\")\n",
        "\n",
        "        try:\n",
        "            # Inicializar cliente Dask\n",
        "            client = Client(n_workers=n_workers, threads_per_worker=threads_per_worker,\n",
        "                          silence_logs=30, dashboard_address=None)\n",
        "\n",
        "            # Convertir a Dask DataFrame\n",
        "            df_dask = dd.from_pandas(\n",
        "                pd.DataFrame(X_synthetic),\n",
        "                npartitions=n_workers * threads_per_worker\n",
        "            )\n",
        "\n",
        "            # Medir tiempo de operaciones típicas\n",
        "            start_time = time.time()\n",
        "            memory_before = psutil.virtual_memory().used / 1e9\n",
        "\n",
        "            # Operaciones de prueba\n",
        "            result1 = df_dask.mean().compute()\n",
        "            result2 = df_dask.std().compute()\n",
        "            result3 = (df_dask * 2).sum().compute()\n",
        "\n",
        "            execution_time = time.time() - start_time\n",
        "            memory_after = psutil.virtual_memory().used / 1e9\n",
        "            memory_used = memory_after - memory_before\n",
        "\n",
        "            # Calcular métricas de escalabilidad\n",
        "            total_cores = n_workers * threads_per_worker\n",
        "\n",
        "            scalability_results.append({\n",
        "                'n_workers': n_workers,\n",
        "                'threads_per_worker': threads_per_worker,\n",
        "                'total_cores': total_cores,\n",
        "                'execution_time': execution_time,\n",
        "                'memory_used_gb': memory_used,\n",
        "                'throughput': n_samples / execution_time if execution_time > 0 else 0\n",
        "            })\n",
        "\n",
        "            logger.log(f\"Configuración {n_workers}x{threads_per_worker}: {execution_time:.2f}s, {memory_used:.2f}GB\")\n",
        "\n",
        "            client.close()\n",
        "            time.sleep(2)  # Pausa para limpieza\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.log(f\"Error en configuración {n_workers}x{threads_per_worker}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Guardar resultados\n",
        "    scalability_df = pd.DataFrame(scalability_results)\n",
        "    scalability_df.to_csv(f'{RESULTS_DIR}/scalability_analysis.csv', index=False)\n",
        "\n",
        "    return scalability_results\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 8. VISUALIZACIONES COMPREHENSIVAS\n",
        "# -----------------------------------------------------------\n",
        "def create_comprehensive_visualizations(results, trained_models, y_test, scalability_results, comparison_results):\n",
        "    logger.log(\"Creando visualizaciones comprehensivas para el paper...\")\n",
        "\n",
        "    # 1. Comparación de modelos - Métricas principales\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "    model_names = list(results.keys())\n",
        "    rmse_values = [results[m]['rmse'] for m in model_names]\n",
        "    mae_values = [results[m]['mae'] for m in model_names]\n",
        "    r2_values = [results[m]['r2'] for m in model_names]\n",
        "    times = [results[m]['training_time'] for m in model_names]\n",
        "\n",
        "    # RMSE Comparison\n",
        "    bars1 = axes[0,0].bar(model_names, rmse_values, color='skyblue', edgecolor='navy', alpha=0.7)\n",
        "    axes[0,0].set_ylabel('RMSE')\n",
        "    axes[0,0].set_title('Comparación de RMSE por Modelo')\n",
        "    axes[0,0].tick_params(axis='x', rotation=45)\n",
        "    for bar, val in zip(bars1, rmse_values):\n",
        "        axes[0,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
        "                      f'{val:.2f}', ha='center', va='bottom')\n",
        "\n",
        "    # R² Comparison\n",
        "    bars2 = axes[0,1].bar(model_names, r2_values, color='lightcoral', edgecolor='darkred', alpha=0.7)\n",
        "    axes[0,1].set_ylabel('R² Score')\n",
        "    axes[0,1].set_title('Comparación de R² por Modelo')\n",
        "    axes[0,1].tick_params(axis='x', rotation=45)\n",
        "    axes[0,1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "    for bar, val in zip(bars2, r2_values):\n",
        "        axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                      f'{val:.3f}', ha='center', va='bottom')\n",
        "\n",
        "    # MAE Comparison\n",
        "    bars3 = axes[1,0].bar(model_names, mae_values, color='lightgreen', edgecolor='darkgreen', alpha=0.7)\n",
        "    axes[1,0].set_ylabel('MAE')\n",
        "    axes[1,0].set_title('Comparación de MAE por Modelo')\n",
        "    axes[1,0].tick_params(axis='x', rotation=45)\n",
        "    for bar, val in zip(bars3, mae_values):\n",
        "        axes[1,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
        "                      f'{val:.2f}', ha='center', va='bottom')\n",
        "\n",
        "    # Training Time Comparison\n",
        "    bars4 = axes[1,1].bar(model_names, times, color='gold', edgecolor='orange', alpha=0.7)\n",
        "    axes[1,1].set_ylabel('Tiempo de Entrenamiento (s)')\n",
        "    axes[1,1].set_title('Tiempo de Entrenamiento por Modelo')\n",
        "    axes[1,1].tick_params(axis='x', rotation=45)\n",
        "    for bar, val in zip(bars4, times):\n",
        "        axes[1,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
        "                      f'{val:.1f}s', ha='center', va='bottom')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{PLOTS_DIR}/model_comparison_comprehensive.png', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # 2. Predicciones vs Valores Reales (mejor modelo)\n",
        "    best_model_name = min(results.keys(), key=lambda x: results[x]['rmse'])\n",
        "    best_predictions = results[best_model_name]['predictions']\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "    # Scatter plot\n",
        "    axes[0].scatter(y_test, best_predictions, alpha=0.6, s=20)\n",
        "    axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2)\n",
        "    axes[0].set_xlabel('RUL Real')\n",
        "    axes[0].set_ylabel('RUL Predicho')\n",
        "    axes[0].set_title(f'Predicciones vs Real - {best_model_name}')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Residual plot\n",
        "    residuals = y_test - best_predictions\n",
        "    axes[1].scatter(best_predictions, residuals, alpha=0.6, s=20)\n",
        "    axes[1].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
        "    axes[1].set_xlabel('RUL Predicho')\n",
        "    axes[1].set_ylabel('Residuales')\n",
        "    axes[1].set_title('Análisis de Residuales')\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Error distribution\n",
        "    axes[2].hist(residuals, bins=30, alpha=0.7, edgecolor='black')\n",
        "    axes[2].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
        "    axes[2].set_xlabel('Error (Real - Predicho)')\n",
        "    axes[2].set_ylabel('Frecuencia')\n",
        "    axes[2].set_title('Distribución del Error')\n",
        "    axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{PLOTS_DIR}/best_model_analysis.png', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # 3. Análisis de Escalabilidad\n",
        "    if scalability_results:\n",
        "        scalability_df = pd.DataFrame(scalability_results)\n",
        "\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "        # Tiempo vs Cores\n",
        "        axes[0].plot(scalability_df['total_cores'], scalability_df['execution_time'],\n",
        "                    'bo-', linewidth=2, markersize=8)\n",
        "        axes[0].set_xlabel('Número Total de Cores')\n",
        "        axes[0].set_ylabel('Tiempo de Ejecución (s)')\n",
        "        axes[0].set_title('Escalabilidad: Tiempo vs Cores')\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Throughput vs Cores\n",
        "        axes[1].plot(scalability_df['total_cores'], scalability_df['throughput'],\n",
        "                    'go-', linewidth=2, markersize=8)\n",
        "        axes[1].set_xlabel('Número Total de Cores')\n",
        "        axes[1].set_ylabel('Throughput (muestras/s)')\n",
        "        axes[1].set_title('Escalabilidad: Throughput vs Cores')\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "        # Memoria vs Cores\n",
        "        axes[2].plot(scalability_df['total_cores'], scalability_df['memory_used_gb'],\n",
        "                    'ro-', linewidth=2, markersize=8)\n",
        "        axes[2].set_xlabel('Número Total de Cores')\n",
        "        axes[2].set_ylabel('Memoria Utilizada (GB)')\n",
        "        axes[2].set_title('Uso de Memoria vs Cores')\n",
        "        axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{PLOTS_DIR}/scalability_analysis.png', bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    # 4. Comparación Dask vs Pandas\n",
        "    if comparison_results:\n",
        "        categories = ['Tiempo (s)', 'Memoria (GB)', 'RMSE']\n",
        "        pandas_values = [comparison_results['pandas']['time'],\n",
        "                        comparison_results['pandas']['memory_gb'],\n",
        "                        comparison_results['pandas']['rmse']]\n",
        "        dask_values = [comparison_results['dask']['time'],\n",
        "                      comparison_results['dask']['memory_gb'],\n",
        "                      comparison_results['dask']['rmse']]\n",
        "\n",
        "        x = np.arange(len(categories))\n",
        "        width = 0.35\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(12, 8))\n",
        "        bars1 = ax.bar(x - width/2, pandas_values, width, label='Pandas (Eager)',\n",
        "                      color='lightblue', edgecolor='navy', alpha=0.7)\n",
        "        bars2 = ax.bar(x + width/2, dask_values, width, label='Dask (Lazy)',\n",
        "                      color='lightcoral', edgecolor='darkred', alpha=0.7)\n",
        "\n",
        "        ax.set_xlabel('Métricas')\n",
        "        ax.set_ylabel('Valores')\n",
        "        ax.set_title('Comparación Rendimiento: Dask vs Pandas')\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(categories)\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Añadir valores en las barras\n",
        "        for bars in [bars1, bars2]:\n",
        "            for bar in bars:\n",
        "                height = bar.get_height()\n",
        "                ax.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
        "                       f'{height:.3f}', ha='center', va='bottom')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{PLOTS_DIR}/dask_vs_pandas_comparison.png', bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    # 5. Cross-validation results\n",
        "    cv_means = [results[m]['cv_rmse_mean'] for m in model_names]\n",
        "    cv_stds = [results[m]['cv_rmse_std'] for m in model_names]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    bars = ax.bar(model_names, cv_means, yerr=cv_stds, capsize=5,\n",
        "                 color='mediumpurple', edgecolor='indigo', alpha=0.7)\n",
        "    ax.set_ylabel('RMSE (Cross-Validation)')\n",
        "    ax.set_title('Validación Cruzada - RMSE con Intervalos de Confianza')\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    for bar, mean, std in zip(bars, cv_means, cv_stds):\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 1,\n",
        "               f'{mean:.2f}±{std:.2f}', ha='center', va='bottom')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{PLOTS_DIR}/cross_validation_results.png', bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    logger.log(\"Todas las visualizaciones creadas exitosamente\")\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 9. GENERACIÓN DE REPORTE\n",
        "# -----------------------------------------------------------\n",
        "def generate_scientific_report(results, comparison_results, scalability_results, selected_sensors):\n",
        "    logger.log(\"Generando reporte científico completo...\")\n",
        "\n",
        "    report_content = f\"\"\"\n",
        "# REPORTE CIENTÍFICO: PREDICCIÓN DISTRIBUIDA DE VIDA ÚTIL REMANENTE (RUL)\n",
        "## Análisis Comparativo de Dask vs Enfoques Tradicionales\n",
        "\n",
        "**Fecha de generación:** {datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
        "\n",
        "## RESUMEN EJECUTIVO\n",
        "\n",
        "Este estudio evalúa la eficacia de Dask para el procesamiento distribuido en la predicción de vida útil remanente (RUL) de motores turbofan utilizando el dataset C-MAPSS de la NASA.\n",
        "\n",
        "### Hallazgos Principales:\n",
        "\"\"\"\n",
        "\n",
        "    # Encontrar el mejor modelo\n",
        "    best_model = min(results.keys(), key=lambda x: results[x]['rmse'])\n",
        "    best_rmse = results[best_model]['rmse']\n",
        "    best_r2 = results[best_model]['r2']\n",
        "\n",
        "    report_content += f\"\"\"\n",
        "- **Mejor modelo:** {best_model} (RMSE: {best_rmse:.4f}, R²: {best_r2:.4f})\n",
        "- **Sensores seleccionados:** {len(selected_sensors)} de 21 disponibles\n",
        "- **Características generadas:** {len([c for c in results[best_model].get('feature_names', []) if c])} características engineered\n",
        "\"\"\"\n",
        "\n",
        "    if comparison_results:\n",
        "        pandas_time = comparison_results['pandas']['time']\n",
        "        dask_time = comparison_results['dask']['time']\n",
        "        speedup = pandas_time / dask_time if dask_time > 0 else 1\n",
        "\n",
        "        report_content += f\"\"\"\n",
        "- **Speedup Dask vs Pandas:** {speedup:.2f}x\n",
        "- **Eficiencia de memoria:** Dask utilizó {comparison_results['dask']['memory_gb']:.2f}GB vs {comparison_results['pandas']['memory_gb']:.2f}GB de Pandas\n",
        "\"\"\"\n",
        "\n",
        "    report_content += f\"\"\"\n",
        "\n",
        "## METODOLOGÍA\n",
        "\n",
        "### Preprocesamiento de Datos:\n",
        "1. **Eliminación de variables constantes:** Variables con varianza cero o casi cero fueron removidas\n",
        "2. **Selección de sensores:** Utilizando información mutua, se seleccionaron los {len(selected_sensors)} sensores más informativos\n",
        "3. **Ingeniería de características:** Se crearon características de ventana móvil (3, 5, 10, 15 ciclos), tendencias y estadísticas acumulativas\n",
        "\n",
        "### Modelos Evaluados:\n",
        "\"\"\"\n",
        "\n",
        "    for model_name, model_results in results.items():\n",
        "        report_content += f\"\"\"\n",
        "#### {model_name}:\n",
        "- RMSE: {model_results['rmse']:.4f}\n",
        "- MAE: {model_results['mae']:.4f}\n",
        "- R²: {model_results['r2']:.4f}\n",
        "- Tiempo entrenamiento: {model_results['training_time']:.2f}s\n",
        "- CV RMSE: {model_results['cv_rmse_mean']:.4f} ± {model_results['cv_rmse_std']:.4f}\n",
        "- Mejores parámetros: {model_results['best_params']}\n",
        "\"\"\"\n",
        "\n",
        "    if scalability_results:\n",
        "        report_content += f\"\"\"\n",
        "\n",
        "## ANÁLISIS DE ESCALABILIDAD\n",
        "\n",
        "Se evaluaron diferentes configuraciones de workers y threads:\n",
        "\n",
        "| Workers | Threads/Worker | Total Cores | Tiempo (s) | Memoria (GB) | Throughput |\n",
        "|---------|----------------|-------------|------------|--------------|------------|\n",
        "\"\"\"\n",
        "        for result in scalability_results:\n",
        "            report_content += f\"| {result['n_workers']} | {result['threads_per_worker']} | {result['total_cores']} | {result['execution_time']:.2f} | {result['memory_used_gb']:.2f} | {result['throughput']:.0f} |\\n\"\n",
        "\n",
        "    if comparison_results:\n",
        "        report_content += f\"\"\"\n",
        "\n",
        "## COMPARACIÓN DASK VS PANDAS\n",
        "\n",
        "| Métrica | Pandas (Eager) | Dask (Lazy) | Mejora |\n",
        "|---------|----------------|-------------|---------|\n",
        "| Tiempo | {comparison_results['pandas']['time']:.2f}s | {comparison_results['dask']['time']:.2f}s | {pandas_time/dask_time:.2f}x |\n",
        "| Memoria | {comparison_results['pandas']['memory_gb']:.2f}GB | {comparison_results['dask']['memory_gb']:.2f}GB | {comparison_results['pandas']['memory_gb']/comparison_results['dask']['memory_gb']:.2f}x |\n",
        "| RMSE | {comparison_results['pandas']['rmse']:.4f} | {comparison_results['dask']['rmse']:.4f} | - |\n",
        "\"\"\"\n",
        "\n",
        "    report_content += f\"\"\"\n",
        "\n",
        "## CONCLUSIONES Y RECOMENDACIONES\n",
        "\n",
        "### Rendimiento del Modelo:\n",
        "- El modelo {best_model} mostró el mejor rendimiento con RMSE de {best_rmse:.4f}\n",
        "- La validación cruzada temporal confirma la robustez del modelo\n",
        "- Se requiere mejorar la ingeniería de características para obtener R² positivo\n",
        "\n",
        "### Eficiencia Computacional:\n",
        "- Dask demostró ventajas en el procesamiento de datos distribuido\n",
        "- La escalabilidad horizontal es efectiva hasta cierto punto\n",
        "- El overhead de coordinación se vuelve significativo con configuraciones pequeñas\n",
        "\n",
        "### Recomendaciones para Investigación Futura:\n",
        "1. **Explorar arquitecturas de deep learning** (LSTM, CNN) para capturar patrones temporales complejos\n",
        "2. **Implementar técnicas de ensemble** combinando múltiples modelos\n",
        "3. **Optimizar hiperparámetros** con búsquedas más exhaustivas\n",
        "4. **Evaluar en datasets más grandes** para aprovechar completamente Dask\n",
        "5. **Investigar técnicas de feature selection** más sofisticadas\n",
        "\n",
        "## ARCHIVOS GENERADOS\n",
        "\n",
        "### Modelos:\n",
        "- `{MODEL_DIR}/best_model_{best_model.lower()}.pkl`: Mejor modelo entrenado\n",
        "- `{MODEL_DIR}/scaler.pkl`: Normalizador ajustado\n",
        "\n",
        "### Datos:\n",
        "- `{FEAT_DIR}/train_features_advanced.parquet`: Características de entrenamiento\n",
        "- `{FEAT_DIR}/test_features_advanced.parquet`: Características de prueba\n",
        "- `{FEAT_DIR}/mutual_information_analysis.csv`: Análisis de selección de sensores\n",
        "\n",
        "### Resultados:\n",
        "- `{RESULTS_DIR}/comprehensive_results.json`: Todos los resultados numéricos\n",
        "- `{RESULTS_DIR}/scalability_analysis.csv`: Resultados de escalabilidad\n",
        "- `{PLOTS_DIR}/`: Todas las visualizaciones generadas\n",
        "\n",
        "### Visualizaciones Clave:\n",
        "- `model_comparison_comprehensive.png`: Comparación entre todos los modelos\n",
        "- `best_model_analysis.png`: Análisis detallado del mejor modelo\n",
        "- `scalability_analysis.png`: Resultados de escalabilidad\n",
        "- `dask_vs_pandas_comparison.png`: Comparación de enfoques\n",
        "- `mutual_information_analysis.png`: Selección de sensores\n",
        "- `correlation_matrix_selected_sensors.png`: Correlaciones entre variables\n",
        "\n",
        "---\n",
        "*Reporte generado automáticamente por el sistema de análisis distribuido*\n",
        "\"\"\"\n",
        "\n",
        "    # Guardar reporte\n",
        "    with open(f'{RESULTS_DIR}/scientific_report.md', 'w', encoding='utf-8') as f:\n",
        "        f.write(report_content)\n",
        "\n",
        "    # Guardar resultados completos en JSON\n",
        "    complete_results = {\n",
        "        'experiment_timestamp': datetime.datetime.now().isoformat(),\n",
        "        'model_results': results,\n",
        "        'comparison_results': comparison_results,\n",
        "        'scalability_results': scalability_results,\n",
        "        'selected_sensors': selected_sensors,\n",
        "        'best_model': best_model,\n",
        "        'dataset_info': {\n",
        "            'name': 'C-MAPSS FD001',\n",
        "            'source': 'NASA Prognostics Center of Excellence'\n",
        "        }\n",
        "    }\n",
        "\n",
        "    with open(f'{RESULTS_DIR}/comprehensive_results.json', 'w') as f:\n",
        "        json.dump(complete_results, f, indent=2, default=str)\n",
        "\n",
        "    logger.log(\"Reporte científico generado exitosamente\")\n",
        "    return report_content\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 10. FUNCIÓN PRINCIPAL\n",
        "# -----------------------------------------------------------\n",
        "def main():\n",
        "    \"\"\"Función principal que ejecuta todo el pipeline científico\"\"\"\n",
        "\n",
        "    logger.log(\"=== INICIANDO PIPELINE CIENTÍFICO COMPLETO ===\")\n",
        "    total_start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        # Etapa 1: Carga y preprocesamiento\n",
        "        logger.log(\"\\n ETAPA 1: CARGA Y PREPROCESAMIENTO\")\n",
        "        train_dd, test_dd, rul = load_and_preprocess_data()\n",
        "        if train_dd is None:\n",
        "            logger.log(\" Error: No se pudieron cargar los datos\")\n",
        "            return\n",
        "\n",
        "        # Recargar desde archivos limpios como DataFrames de pandas\n",
        "        train_features_path = f'{CLEAN}/train_FD001_clean.parquet'\n",
        "        test_features_path = f'{CLEAN}/test_FD001_clean.parquet'\n",
        "\n",
        "        # Leer como pandas primero, luego convertir a Dask si es necesario\n",
        "        if os.path.exists(train_features_path):\n",
        "            train = pd.read_parquet(train_features_path)\n",
        "            test = pd.read_parquet(test_features_path)\n",
        "        else:\n",
        "            train = train_dd.compute()\n",
        "            test = test_dd.compute()\n",
        "\n",
        "        # Etapa 2: Ingeniería de características\n",
        "        logger.log(\"\\n ETAPA 2: INGENIERÍA DE CARACTERÍSTICAS\")\n",
        "        train_features, test_features, selected_sensors = advanced_feature_engineering(train, test, rul)\n",
        "\n",
        "        # Etapa 3: Análisis exploratorio\n",
        "        logger.log(\"\\n ETAPA 3: ANÁLISIS EXPLORATORIO DE DATOS\")\n",
        "        comprehensive_eda(train_features, selected_sensors)\n",
        "\n",
        "        # Etapa 4: Modelado comprehensivo\n",
        "        logger.log(\"\\n ETAPA 4: MODELADO Y EVALUACIÓN\")\n",
        "        results, trained_models, y_test, scaler = comprehensive_modeling(train_features, test_features)\n",
        "\n",
        "        # Etapa 5: Comparación Dask vs Pandas\n",
        "        logger.log(\"\\n ETAPA 5: COMPARACIÓN DASK VS PANDAS\")\n",
        "        comparison_results = dask_vs_pandas_comparison(train_features, test_features)\n",
        "\n",
        "        # Etapa 6: Análisis de escalabilidad\n",
        "        logger.log(\"\\n ETAPA 6: ANÁLISIS DE ESCALABILIDAD\")\n",
        "        scalability_results = scalability_analysis()\n",
        "\n",
        "        # Etapa 7: Visualizaciones comprehensivas\n",
        "        logger.log(\"\\n ETAPA 7: GENERACIÓN DE VISUALIZACIONES\")\n",
        "        create_comprehensive_visualizations(results, trained_models, y_test,\n",
        "                                          scalability_results, comparison_results)\n",
        "\n",
        "        # Etapa 8: Guardar modelos\n",
        "        logger.log(\"\\n ETAPA 8: GUARDADO DE MODELOS\")\n",
        "        best_model_name = min(results.keys(), key=lambda x: results[x]['rmse'])\n",
        "        best_model = trained_models[best_model_name]\n",
        "\n",
        "        joblib.dump(best_model, f'{MODEL_DIR}/best_model_{best_model_name.lower()}.pkl')\n",
        "        joblib.dump(scaler, f'{MODEL_DIR}/scaler.pkl')\n",
        "\n",
        "        # Etapa 9: Reporte científico\n",
        "        logger.log(\"\\n ETAPA 9: GENERACIÓN DE REPORTE CIENTÍFICO\")\n",
        "        report = generate_scientific_report(results, comparison_results,\n",
        "                                          scalability_results, selected_sensors)\n",
        "\n",
        "        # Resumen final\n",
        "        total_time = time.time() - total_start_time\n",
        "        logger.log(f\"\\n PIPELINE COMPLETADO EXITOSAMENTE\")\n",
        "        logger.log(f\" Tiempo total: {total_time:.2f} segundos\")\n",
        "        logger.log(f\" Resultados guardados en: {RESULTS_DIR}\")\n",
        "        logger.log(f\" Mejor modelo: {best_model_name} (RMSE: {results[best_model_name]['rmse']:.4f})\")\n",
        "\n",
        "        # Mostrar estructura de archivos generados\n",
        "        logger.log(\"\\n ARCHIVOS GENERADOS:\")\n",
        "        for root, dirs, files in os.walk(RESULTS_DIR):\n",
        "            level = root.replace(RESULTS_DIR, '').count(os.sep)\n",
        "            indent = ' ' * 2 * level\n",
        "            logger.log(f\"{indent}{os.path.basename(root)}/\")\n",
        "            subindent = ' ' * 2 * (level + 1)\n",
        "            for file in files:\n",
        "                logger.log(f\"{subindent}{file}\")\n",
        "\n",
        "        return {\n",
        "            'success': True,\n",
        "            'results': results,\n",
        "            'best_model': best_model_name,\n",
        "            'total_time': total_time,\n",
        "            'output_dir': RESULTS_DIR\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.log(f\" ERROR CRÍTICO: {str(e)}\")\n",
        "        import traceback\n",
        "        logger.log(traceback.format_exc())\n",
        "        return {'success': False, 'error': str(e)}\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# EJECUTAR EXPERIMENTO\n",
        "# -----------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Configurar warnings\n",
        "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "    print(\" Iniciando Experimento Científico - Predicción RUL con Dask\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Ejecutar pipeline completo\n",
        "    final_results = main()\n",
        "\n",
        "    if final_results['success']:\n",
        "        print(\"\\n ¡EXPERIMENTO COMPLETADO CON ÉXITO!\")\n",
        "        print(f\" Mejor modelo: {final_results['best_model']}\")\n",
        "        print(f\" Tiempo total: {final_results['total_time']:.2f}s\")\n",
        "        print(f\" Resultados en: {final_results['output_dir']}\")\n",
        "    else:\n",
        "        print(f\"\\n EXPERIMENTO FALLÓ: {final_results['error']}\")\n",
        "\n",
        "    print(\"=\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQbwBTdD3yR9",
        "outputId": "a99bf9c7-26dd-4550-c646-63f6397d275f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-08-03 17:32:32] === INICIANDO EXPERIMENTO CIENTÍFICO ===\n",
            "🚀 Iniciando Experimento Científico - Predicción RUL con Dask\n",
            "============================================================\n",
            "[2025-08-03 17:32:32] === INICIANDO PIPELINE CIENTÍFICO COMPLETO ===\n",
            "[2025-08-03 17:32:32] \n",
            "🔄 ETAPA 1: CARGA Y PREPROCESAMIENTO\n",
            "[2025-08-03 17:32:32] Iniciando carga y preprocesamiento de datos...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 219f07eb6c02333fc71458e625f8fed4 initialized by task ('shuffle-transfer-219f07eb6c02333fc71458e625f8fed4', 0) executed on worker tcp://127.0.0.1:44601\n",
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle 219f07eb6c02333fc71458e625f8fed4 deactivated due to stimulus 'task-finished-1754242352.548572'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-08-03 17:32:32] Datos cargados - Train: 20631 filas, Test: 13096 filas\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle acb682be53394a534a381bb3f36b1576 initialized by task ('shuffle-transfer-acb682be53394a534a381bb3f36b1576', 0) executed on worker tcp://127.0.0.1:44601\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-08-03 17:32:32] Columnas constantes encontradas: 7\n",
            "[2025-08-03 17:32:32] Columnas casi constantes encontradas: 3\n",
            "[2025-08-03 17:32:32] Eliminadas 10 columnas problemáticas\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:distributed.shuffle._scheduler_plugin:Shuffle acb682be53394a534a381bb3f36b1576 deactivated due to stimulus 'task-finished-1754242352.8125222'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-08-03 17:32:33] \n",
            "🔄 ETAPA 2: INGENIERÍA DE CARACTERÍSTICAS\n",
            "[2025-08-03 17:32:33] Iniciando ingeniería de características avanzada...\n",
            "[2025-08-03 17:32:33] Trabajando con pandas DataFrames...\n",
            "[2025-08-03 17:32:33] Sensores seleccionados: 4 de 13\n",
            "[2025-08-03 17:32:33] Aplicando transformaciones de características...\n",
            "[2025-08-03 17:32:34] Características creadas - Train: (20631, 95), Test: (13096, 95)\n",
            "[2025-08-03 17:32:34] \n",
            "🔄 ETAPA 3: ANÁLISIS EXPLORATORIO DE DATOS\n",
            "[2025-08-03 17:32:34] Realizando análisis exploratorio de datos...\n",
            "[2025-08-03 17:32:38] Análisis exploratorio completado\n",
            "[2025-08-03 17:32:38] \n",
            "🔄 ETAPA 4: MODELADO Y EVALUACIÓN\n",
            "[2025-08-03 17:32:38] Iniciando modelado comprehensivo...\n",
            "[2025-08-03 17:32:38] Características utilizadas: 92\n",
            "[2025-08-03 17:32:38] Datos de entrenamiento: (20631, 92)\n",
            "[2025-08-03 17:32:38] Datos de prueba: (13096, 92)\n",
            "[2025-08-03 17:32:38] \n",
            "--- Entrenando LightGBM ---\n",
            "[2025-08-03 18:57:12] \n",
            "=== LightGBM Resultados ===\n",
            "[2025-08-03 18:57:12] RMSE: 175.947890\n",
            "[2025-08-03 18:57:12] MAE: 148.134198\n",
            "[2025-08-03 18:57:12] R²: -4.550614\n",
            "[2025-08-03 18:57:12] CV RMSE (mean±std): 38.824±12.934\n",
            "[2025-08-03 18:57:12] Tiempo entrenamiento: 5074.47s\n",
            "[2025-08-03 18:57:12] \n",
            "--- Entrenando XGBoost ---\n",
            "[2025-08-03 19:16:20] \n",
            "=== XGBoost Resultados ===\n",
            "[2025-08-03 19:16:20] RMSE: 176.085417\n",
            "[2025-08-03 19:16:20] MAE: 148.274872\n",
            "[2025-08-03 19:16:20] R²: -4.559294\n",
            "[2025-08-03 19:16:20] CV RMSE (mean±std): 38.897±12.898\n",
            "[2025-08-03 19:16:20] Tiempo entrenamiento: 1147.13s\n",
            "[2025-08-03 19:16:20] \n",
            "--- Entrenando RandomForest ---\n",
            "[2025-08-03 20:56:07] \n",
            "=== RandomForest Resultados ===\n",
            "[2025-08-03 20:56:07] RMSE: 175.758737\n",
            "[2025-08-03 20:56:07] MAE: 147.494117\n",
            "[2025-08-03 20:56:07] R²: -4.538686\n",
            "[2025-08-03 20:56:07] CV RMSE (mean±std): 40.193±12.967\n",
            "[2025-08-03 20:56:07] Tiempo entrenamiento: 5987.01s\n",
            "[2025-08-03 20:56:07] \n",
            "--- Entrenando Ridge ---\n",
            "[2025-08-03 20:56:09] \n",
            "=== Ridge Resultados ===\n",
            "[2025-08-03 20:56:09] RMSE: 174.122737\n",
            "[2025-08-03 20:56:09] MAE: 148.407183\n",
            "[2025-08-03 20:56:09] R²: -4.436055\n",
            "[2025-08-03 20:56:09] CV RMSE (mean±std): 41.047±12.450\n",
            "[2025-08-03 20:56:09] Tiempo entrenamiento: 2.44s\n",
            "[2025-08-03 20:56:09] \n",
            "🔄 ETAPA 5: COMPARACIÓN DASK VS PANDAS\n",
            "[2025-08-03 20:56:09] Comparando rendimiento Dask vs Pandas...\n",
            "[2025-08-03 20:56:09] Probando enfoque Pandas (eager)...\n",
            "[2025-08-03 20:56:10] Probando enfoque Dask (lazy)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:distributed.scheduler:State start\n",
            "INFO:distributed.scheduler:  Scheduler at:     tcp://127.0.0.1:37859\n",
            "INFO:distributed.scheduler:  dashboard at:  http://127.0.0.1:40449/status\n",
            "INFO:distributed.scheduler:Registering Worker plugin shuffle\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:39903'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-08-03 20:56:18] \n",
            "=== Comparación Pandas vs Dask ===\n",
            "[2025-08-03 20:56:18] Pandas - Tiempo: 1.24s\n",
            "[2025-08-03 20:56:18] Pandas - Memoria: 0.00GB\n",
            "[2025-08-03 20:56:18] Pandas - RMSE: 176.7530\n",
            "[2025-08-03 20:56:18] Dask - Tiempo: 7.35s\n",
            "[2025-08-03 20:56:18] Dask - Memoria: 0.09GB\n",
            "[2025-08-03 20:56:18] Dask - RMSE: 176.0701\n",
            "[2025-08-03 20:56:18] Speedup: 0.17x\n",
            "[2025-08-03 20:56:18] \n",
            "🔄 ETAPA 6: ANÁLISIS DE ESCALABILIDAD\n",
            "[2025-08-03 20:56:18] Realizando análisis de escalabilidad...\n",
            "[2025-08-03 20:56:18] Probando configuración: 1 workers, 1 threads c/u\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:37985 name: 0\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:37985\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:48628\n",
            "INFO:distributed.scheduler:Receive client connection: Client-4d6c534b-70ac-11f0-8105-0242ac1c000c\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:48630\n",
            "INFO:distributed.scheduler:Remove client Client-4d6c534b-70ac-11f0-8105-0242ac1c000c\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:48630; closing.\n",
            "INFO:distributed.scheduler:Remove client Client-4d6c534b-70ac-11f0-8105-0242ac1c000c\n",
            "INFO:distributed.scheduler:Close client connection: Client-4d6c534b-70ac-11f0-8105-0242ac1c000c\n",
            "INFO:distributed.scheduler:Retire worker addresses (stimulus_id='retire-workers-1754254581.2234986') (0,)\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:39903'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:48628; closing.\n",
            "INFO:distributed.scheduler:Remove worker addr: tcp://127.0.0.1:37985 name: 0 (stimulus_id='handle-worker-cleanup-1754254581.2313604')\n",
            "INFO:distributed.scheduler:Lost all workers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-08-03 20:56:21] Configuración 1x1: 2.02s, 0.18GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:distributed.nanny:Nanny at 'tcp://127.0.0.1:39903' closed.\n",
            "INFO:distributed.scheduler:Closing scheduler. Reason: unknown\n",
            "INFO:distributed.scheduler:Scheduler closing all comms\n",
            "INFO:distributed.scheduler:State start\n",
            "INFO:distributed.scheduler:  Scheduler at:     tcp://127.0.0.1:45807\n",
            "INFO:distributed.scheduler:  dashboard at:  http://127.0.0.1:40941/status\n",
            "INFO:distributed.scheduler:Registering Worker plugin shuffle\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:38017'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:41795'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-08-03 20:56:23] Probando configuración: 2 workers, 1 threads c/u\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:33625 name: 0\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:33625\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:54682\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:42525 name: 1\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:42525\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:54690\n",
            "INFO:distributed.scheduler:Receive client connection: Client-507cc356-70ac-11f0-8105-0242ac1c000c\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:54698\n",
            "INFO:distributed.scheduler:Remove client Client-507cc356-70ac-11f0-8105-0242ac1c000c\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:54698; closing.\n",
            "INFO:distributed.scheduler:Remove client Client-507cc356-70ac-11f0-8105-0242ac1c000c\n",
            "INFO:distributed.scheduler:Close client connection: Client-507cc356-70ac-11f0-8105-0242ac1c000c\n",
            "INFO:distributed.scheduler:Retire worker addresses (stimulus_id='retire-workers-1754254586.7860065') (0, 1)\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:38017'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:41795'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:54682; closing.\n",
            "INFO:distributed.scheduler:Remove worker addr: tcp://127.0.0.1:33625 name: 0 (stimulus_id='handle-worker-cleanup-1754254586.7951694')\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:54690; closing.\n",
            "INFO:distributed.scheduler:Remove worker addr: tcp://127.0.0.1:42525 name: 1 (stimulus_id='handle-worker-cleanup-1754254586.8001668')\n",
            "INFO:distributed.scheduler:Lost all workers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-08-03 20:56:26] Configuración 2x1: 2.40s, 0.30GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:distributed.nanny:Nanny at 'tcp://127.0.0.1:38017' closed.\n",
            "INFO:distributed.nanny:Nanny at 'tcp://127.0.0.1:41795' closed.\n",
            "INFO:distributed.scheduler:Closing scheduler. Reason: unknown\n",
            "INFO:distributed.scheduler:Scheduler closing all comms\n",
            "INFO:distributed.scheduler:State start\n",
            "INFO:distributed.scheduler:  Scheduler at:     tcp://127.0.0.1:35701\n",
            "INFO:distributed.scheduler:  dashboard at:  http://127.0.0.1:34975/status\n",
            "INFO:distributed.scheduler:Registering Worker plugin shuffle\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:35735'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:39493'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-08-03 20:56:29] Probando configuración: 2 workers, 2 threads c/u\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:39681 name: 0\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:39681\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:48694\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:42177 name: 1\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:42177\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:48696\n",
            "INFO:distributed.scheduler:Receive client connection: Client-53e892e6-70ac-11f0-8105-0242ac1c000c\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:48708\n",
            "INFO:distributed.scheduler:Remove client Client-53e892e6-70ac-11f0-8105-0242ac1c000c\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:48708; closing.\n",
            "INFO:distributed.scheduler:Remove client Client-53e892e6-70ac-11f0-8105-0242ac1c000c\n",
            "INFO:distributed.scheduler:Close client connection: Client-53e892e6-70ac-11f0-8105-0242ac1c000c\n",
            "INFO:distributed.scheduler:Retire worker addresses (stimulus_id='retire-workers-1754254593.3135862') (0, 1)\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:35735'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:39493'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:48696; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:48694; closing.\n",
            "INFO:distributed.scheduler:Remove worker addr: tcp://127.0.0.1:42177 name: 1 (stimulus_id='handle-worker-cleanup-1754254593.327661')\n",
            "INFO:distributed.scheduler:Remove worker addr: tcp://127.0.0.1:39681 name: 0 (stimulus_id='handle-worker-cleanup-1754254593.3299656')\n",
            "INFO:distributed.scheduler:Lost all workers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-08-03 20:56:33] Configuración 2x2: 2.76s, 0.32GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:distributed.nanny:Nanny at 'tcp://127.0.0.1:39493' closed.\n",
            "INFO:distributed.nanny:Nanny at 'tcp://127.0.0.1:35735' closed.\n",
            "INFO:distributed.scheduler:Closing scheduler. Reason: unknown\n",
            "INFO:distributed.scheduler:Scheduler closing all comms\n",
            "INFO:distributed.scheduler:State start\n",
            "INFO:distributed.scheduler:  Scheduler at:     tcp://127.0.0.1:32967\n",
            "INFO:distributed.scheduler:  dashboard at:  http://127.0.0.1:46179/status\n",
            "INFO:distributed.scheduler:Registering Worker plugin shuffle\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:35921'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:37789'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-08-03 20:56:35] Probando configuración: 4 workers, 1 threads c/u\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:41689'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:34669'\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:33129 name: 0\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:33129\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:39122\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:32919 name: 3\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:32919\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:39112\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:42957 name: 2\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:42957\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:39114\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:45963 name: 1\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:45963\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:39138\n",
            "INFO:distributed.scheduler:Receive client connection: Client-57c1ad2c-70ac-11f0-8105-0242ac1c000c\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:39146\n",
            "INFO:distributed.scheduler:Remove client Client-57c1ad2c-70ac-11f0-8105-0242ac1c000c\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:39146; closing.\n",
            "INFO:distributed.scheduler:Remove client Client-57c1ad2c-70ac-11f0-8105-0242ac1c000c\n",
            "INFO:distributed.scheduler:Close client connection: Client-57c1ad2c-70ac-11f0-8105-0242ac1c000c\n",
            "INFO:distributed.scheduler:Retire worker addresses (stimulus_id='retire-workers-1754254603.6771114') (0, 1, 2, 3)\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:35921'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:37789'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:41689'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:34669'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:39122; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:39138; closing.\n",
            "INFO:distributed.scheduler:Remove worker addr: tcp://127.0.0.1:33129 name: 0 (stimulus_id='handle-worker-cleanup-1754254603.701569')\n",
            "INFO:distributed.scheduler:Remove worker addr: tcp://127.0.0.1:45963 name: 1 (stimulus_id='handle-worker-cleanup-1754254603.7036014')\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:39114; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:39112; closing.\n",
            "INFO:distributed.scheduler:Remove worker addr: tcp://127.0.0.1:42957 name: 2 (stimulus_id='handle-worker-cleanup-1754254603.711996')\n",
            "INFO:distributed.scheduler:Remove worker addr: tcp://127.0.0.1:32919 name: 3 (stimulus_id='handle-worker-cleanup-1754254603.7299645')\n",
            "INFO:distributed.scheduler:Lost all workers\n",
            "INFO:distributed.batched:Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:32967 remote=tcp://127.0.0.1:39112>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/distributed/batched.py\", line 115, in _background_send\n",
            "    nbytes = yield coro\n",
            "             ^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 766, in run\n",
            "    value = future.result()\n",
            "            ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/distributed/comm/tcp.py\", line 263, in write\n",
            "    raise CommClosedError()\n",
            "distributed.comm.core.CommClosedError\n",
            "INFO:distributed.batched:Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:32967 remote=tcp://127.0.0.1:39114>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/distributed/batched.py\", line 115, in _background_send\n",
            "    nbytes = yield coro\n",
            "             ^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/gen.py\", line 766, in run\n",
            "    value = future.result()\n",
            "            ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/distributed/comm/tcp.py\", line 263, in write\n",
            "    raise CommClosedError()\n",
            "distributed.comm.core.CommClosedError\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-08-03 20:56:43] Configuración 4x1: 6.25s, 0.50GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:distributed.nanny:Nanny at 'tcp://127.0.0.1:37789' closed.\n",
            "INFO:distributed.nanny:Nanny at 'tcp://127.0.0.1:35921' closed.\n",
            "INFO:distributed.nanny:Nanny at 'tcp://127.0.0.1:34669' closed.\n",
            "INFO:distributed.nanny:Nanny at 'tcp://127.0.0.1:41689' closed.\n",
            "INFO:distributed.scheduler:Closing scheduler. Reason: unknown\n",
            "INFO:distributed.scheduler:Scheduler closing all comms\n",
            "INFO:distributed.scheduler:State start\n",
            "INFO:distributed.scheduler:  Scheduler at:     tcp://127.0.0.1:41633\n",
            "INFO:distributed.scheduler:  dashboard at:  http://127.0.0.1:37449/status\n",
            "INFO:distributed.scheduler:Registering Worker plugin shuffle\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:46841'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:38233'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-08-03 20:56:47] Probando configuración: 4 workers, 2 threads c/u\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:36369'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:44661'\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:36405 name: 3\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:36405\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:45614\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:35075 name: 1\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:35075\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:45620\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:32895 name: 0\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:32895\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:45634\n",
            "INFO:distributed.scheduler:Register worker addr: tcp://127.0.0.1:39271 name: 2\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:39271\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:45636\n",
            "INFO:distributed.scheduler:Receive client connection: Client-5e7bc6de-70ac-11f0-8105-0242ac1c000c\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:45650\n",
            "INFO:distributed.scheduler:Remove client Client-5e7bc6de-70ac-11f0-8105-0242ac1c000c\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:45650; closing.\n",
            "INFO:distributed.scheduler:Remove client Client-5e7bc6de-70ac-11f0-8105-0242ac1c000c\n",
            "INFO:distributed.scheduler:Close client connection: Client-5e7bc6de-70ac-11f0-8105-0242ac1c000c\n",
            "INFO:distributed.scheduler:Retire worker addresses (stimulus_id='retire-workers-1754254613.6774716') (0, 1, 2, 3)\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:46841'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:38233'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:36369'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.nanny:Closing Nanny at 'tcp://127.0.0.1:44661'. Reason: nanny-close\n",
            "INFO:distributed.nanny:Nanny asking worker to close. Reason: nanny-close\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:45634; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:45620; closing.\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:45636; closing.\n",
            "INFO:distributed.scheduler:Remove worker addr: tcp://127.0.0.1:32895 name: 0 (stimulus_id='handle-worker-cleanup-1754254613.7112308')\n",
            "INFO:distributed.scheduler:Remove worker addr: tcp://127.0.0.1:35075 name: 1 (stimulus_id='handle-worker-cleanup-1754254613.713841')\n",
            "INFO:distributed.scheduler:Remove worker addr: tcp://127.0.0.1:39271 name: 2 (stimulus_id='handle-worker-cleanup-1754254613.7160788')\n",
            "INFO:distributed.core:Received 'close-stream' from tcp://127.0.0.1:45614; closing.\n",
            "INFO:distributed.scheduler:Remove worker addr: tcp://127.0.0.1:36405 name: 3 (stimulus_id='handle-worker-cleanup-1754254613.72239')\n",
            "INFO:distributed.scheduler:Lost all workers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-08-03 20:56:53] Configuración 4x2: 4.92s, 0.51GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:distributed.nanny:Nanny at 'tcp://127.0.0.1:38233' closed.\n",
            "INFO:distributed.nanny:Nanny at 'tcp://127.0.0.1:46841' closed.\n",
            "INFO:distributed.nanny:Nanny at 'tcp://127.0.0.1:44661' closed.\n",
            "INFO:distributed.nanny:Nanny at 'tcp://127.0.0.1:36369' closed.\n",
            "INFO:distributed.scheduler:Closing scheduler. Reason: unknown\n",
            "INFO:distributed.scheduler:Scheduler closing all comms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-08-03 20:56:56] \n",
            "🔄 ETAPA 7: GENERACIÓN DE VISUALIZACIONES\n",
            "[2025-08-03 20:56:56] Creando visualizaciones comprehensivas para el paper...\n",
            "[2025-08-03 20:57:01] Todas las visualizaciones creadas exitosamente\n",
            "[2025-08-03 20:57:01] \n",
            "🔄 ETAPA 8: GUARDADO DE MODELOS\n",
            "[2025-08-03 20:57:01] \n",
            "🔄 ETAPA 9: GENERACIÓN DE REPORTE CIENTÍFICO\n",
            "[2025-08-03 20:57:01] Generando reporte científico completo...\n",
            "[2025-08-03 20:57:01] Reporte científico generado exitosamente\n",
            "[2025-08-03 20:57:01] \n",
            "✅ PIPELINE COMPLETADO EXITOSAMENTE\n",
            "[2025-08-03 20:57:01] ⏱️  Tiempo total: 12269.53 segundos\n",
            "[2025-08-03 20:57:01] 📁 Resultados guardados en: /content/drive/MyDrive/CMAPSSData/paper_results\n",
            "[2025-08-03 20:57:01] 🏆 Mejor modelo: Ridge (RMSE: 174.1227)\n",
            "[2025-08-03 20:57:01] \n",
            "📋 ARCHIVOS GENERADOS:\n",
            "[2025-08-03 20:57:01] paper_results/\n",
            "[2025-08-03 20:57:01]   experiment_log.txt\n",
            "[2025-08-03 20:57:01]   scalability_analysis.csv\n",
            "[2025-08-03 20:57:01]   scientific_report.md\n",
            "[2025-08-03 20:57:01]   comprehensive_results.json\n",
            "[2025-08-03 20:57:01]   plots/\n",
            "[2025-08-03 20:57:01]     mutual_information_analysis.png\n",
            "[2025-08-03 20:57:01]     rul_distribution_analysis.png\n",
            "[2025-08-03 20:57:01]     correlation_matrix_selected_sensors.png\n",
            "[2025-08-03 20:57:01]     sensor_temporal_evolution.png\n",
            "[2025-08-03 20:57:01]     model_comparison_comprehensive.png\n",
            "[2025-08-03 20:57:01]     best_model_analysis.png\n",
            "[2025-08-03 20:57:01]     scalability_analysis.png\n",
            "[2025-08-03 20:57:01]     dask_vs_pandas_comparison.png\n",
            "[2025-08-03 20:57:01]     cross_validation_results.png\n",
            "\n",
            "🎉 ¡EXPERIMENTO COMPLETADO CON ÉXITO!\n",
            "📊 Mejor modelo: Ridge\n",
            "⏱️  Tiempo total: 12269.53s\n",
            "📁 Resultados en: /content/drive/MyDrive/CMAPSSData/paper_results\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}